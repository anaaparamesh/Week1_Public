{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kNN-Problems.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anaaparamesh/Week1_Public/blob/master/Notebooks/05_Nearest_Neighbors/NearestNeighbors_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3hHvV20eD58o"
      },
      "source": [
        "# Nearest Neighbors Problem Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z1h_AAj_D4kX",
        "colab": {}
      },
      "source": [
        "# -- imports --\n",
        "import numpy as np\n",
        "import pandas as po\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -- kNN --\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87tmgzNMb9tV",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1\n",
        "\n",
        "Consider the following simple data-set:\n",
        "\n",
        "<img src=\"https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/Images/Week1/knn_notebook_example_table.png\" alt=\"Example Table\" width=\"600\">\n",
        "\n",
        "Now consider the Sample:\n",
        "    $$X= 4, Y = 4, Z = 2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gqjait37Qws0"
      },
      "source": [
        "Using kNN, what is the class for this sample for $k = 1$ and $k = 3?$ Use the Eucledian metric.\n",
        "\n",
        "(YOUR ANSWER HERE) k = 1, 1\n",
        "k = 3, 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MtE0uiKuTWsI"
      },
      "source": [
        "## Problem 2\n",
        "Earlier in the tutorial we were told that kNN depends on several factors, one of them being $k$. Consider the following datasets below, find the optimal value of $k$ that gives the highest accuracy. Visualize your data! Can you come up with some rule for getting a good idea of what $k$ is? \n",
        "\n",
        "HINT: look for a pattern/bound! Answer should be in terms of the size of the dataset $n$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aapncOgUo_5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sovle this problem for each of these datasets\n",
        "from sklearn.datasets import load_iris \n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import load_wine \n",
        "\n",
        "# Load those datasets into some easily accessible variables\n",
        "#The datasets are already normalized, so that saves us some steps!\n",
        "iris = load_iris()                    #iris dataset: size = 150\n",
        "breast_cancer = load_breast_cancer()  #diabetes dataset: size = 569\n",
        "wine = load_wine()                    #wine dataset: size 178\n",
        "\n",
        "# This function will perfom KNN classification for a specified k\n",
        "def split_train_test_dataset(dataset, k, test_size=0.2):\n",
        "    \"\"\"Loads and performs KNN classification on the provided dataset\"\"\"\n",
        "    # Grab and split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        dataset.data, dataset.target, test_size=test_size, random_state=0)\n",
        "\n",
        "    # Build a KNN classifier, fit it and test its predictions\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(\"Validation Accuracy is {:5.1%}\".format(\n",
        "        accuracy_score(y_val, knn.predict(X_val))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQk-b90Kgz-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13c0649b-dd7f-4c69-ccf7-4d829828e5ab"
      },
      "source": [
        "# your code here\n",
        "split_train_test_dataset(iris, 6, test_size=0.2)\n",
        "split_train_test_dataset(breast_cancer, 11, test_size=0.2)\n",
        "split_train_test_dataset(wine, 5, test_size=0.2)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "# Pass multiple numpy arrays as arguments to plt.plot\n",
        "k = np.linspace(5, 30, num=569)\n",
        "print(k)\n",
        "\n",
        "n = np.arange(0, 569)\n",
        "ax.plot(n, k, 'bo') \n",
        "plt.title(\"n vs k\")\n",
        "plt.xlabel(\"n\")\n",
        "plt.ylabel('k')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy is 100.0%\n",
            "Validation Accuracy is 96.5%\n",
            "Validation Accuracy is 80.6%\n",
            "[ 5.          5.04401408  5.08802817  5.13204225  5.17605634  5.22007042\n",
            "  5.26408451  5.30809859  5.35211268  5.39612676  5.44014085  5.48415493\n",
            "  5.52816901  5.5721831   5.61619718  5.66021127  5.70422535  5.74823944\n",
            "  5.79225352  5.83626761  5.88028169  5.92429577  5.96830986  6.01232394\n",
            "  6.05633803  6.10035211  6.1443662   6.18838028  6.23239437  6.27640845\n",
            "  6.32042254  6.36443662  6.4084507   6.45246479  6.49647887  6.54049296\n",
            "  6.58450704  6.62852113  6.67253521  6.7165493   6.76056338  6.80457746\n",
            "  6.84859155  6.89260563  6.93661972  6.9806338   7.02464789  7.06866197\n",
            "  7.11267606  7.15669014  7.20070423  7.24471831  7.28873239  7.33274648\n",
            "  7.37676056  7.42077465  7.46478873  7.50880282  7.5528169   7.59683099\n",
            "  7.64084507  7.68485915  7.72887324  7.77288732  7.81690141  7.86091549\n",
            "  7.90492958  7.94894366  7.99295775  8.03697183  8.08098592  8.125\n",
            "  8.16901408  8.21302817  8.25704225  8.30105634  8.34507042  8.38908451\n",
            "  8.43309859  8.47711268  8.52112676  8.56514085  8.60915493  8.65316901\n",
            "  8.6971831   8.74119718  8.78521127  8.82922535  8.87323944  8.91725352\n",
            "  8.96126761  9.00528169  9.04929577  9.09330986  9.13732394  9.18133803\n",
            "  9.22535211  9.2693662   9.31338028  9.35739437  9.40140845  9.44542254\n",
            "  9.48943662  9.5334507   9.57746479  9.62147887  9.66549296  9.70950704\n",
            "  9.75352113  9.79753521  9.8415493   9.88556338  9.92957746  9.97359155\n",
            " 10.01760563 10.06161972 10.1056338  10.14964789 10.19366197 10.23767606\n",
            " 10.28169014 10.32570423 10.36971831 10.41373239 10.45774648 10.50176056\n",
            " 10.54577465 10.58978873 10.63380282 10.6778169  10.72183099 10.76584507\n",
            " 10.80985915 10.85387324 10.89788732 10.94190141 10.98591549 11.02992958\n",
            " 11.07394366 11.11795775 11.16197183 11.20598592 11.25       11.29401408\n",
            " 11.33802817 11.38204225 11.42605634 11.47007042 11.51408451 11.55809859\n",
            " 11.60211268 11.64612676 11.69014085 11.73415493 11.77816901 11.8221831\n",
            " 11.86619718 11.91021127 11.95422535 11.99823944 12.04225352 12.08626761\n",
            " 12.13028169 12.17429577 12.21830986 12.26232394 12.30633803 12.35035211\n",
            " 12.3943662  12.43838028 12.48239437 12.52640845 12.57042254 12.61443662\n",
            " 12.6584507  12.70246479 12.74647887 12.79049296 12.83450704 12.87852113\n",
            " 12.92253521 12.9665493  13.01056338 13.05457746 13.09859155 13.14260563\n",
            " 13.18661972 13.2306338  13.27464789 13.31866197 13.36267606 13.40669014\n",
            " 13.45070423 13.49471831 13.53873239 13.58274648 13.62676056 13.67077465\n",
            " 13.71478873 13.75880282 13.8028169  13.84683099 13.89084507 13.93485915\n",
            " 13.97887324 14.02288732 14.06690141 14.11091549 14.15492958 14.19894366\n",
            " 14.24295775 14.28697183 14.33098592 14.375      14.41901408 14.46302817\n",
            " 14.50704225 14.55105634 14.59507042 14.63908451 14.68309859 14.72711268\n",
            " 14.77112676 14.81514085 14.85915493 14.90316901 14.9471831  14.99119718\n",
            " 15.03521127 15.07922535 15.12323944 15.16725352 15.21126761 15.25528169\n",
            " 15.29929577 15.34330986 15.38732394 15.43133803 15.47535211 15.5193662\n",
            " 15.56338028 15.60739437 15.65140845 15.69542254 15.73943662 15.7834507\n",
            " 15.82746479 15.87147887 15.91549296 15.95950704 16.00352113 16.04753521\n",
            " 16.0915493  16.13556338 16.17957746 16.22359155 16.26760563 16.31161972\n",
            " 16.3556338  16.39964789 16.44366197 16.48767606 16.53169014 16.57570423\n",
            " 16.61971831 16.66373239 16.70774648 16.75176056 16.79577465 16.83978873\n",
            " 16.88380282 16.9278169  16.97183099 17.01584507 17.05985915 17.10387324\n",
            " 17.14788732 17.19190141 17.23591549 17.27992958 17.32394366 17.36795775\n",
            " 17.41197183 17.45598592 17.5        17.54401408 17.58802817 17.63204225\n",
            " 17.67605634 17.72007042 17.76408451 17.80809859 17.85211268 17.89612676\n",
            " 17.94014085 17.98415493 18.02816901 18.0721831  18.11619718 18.16021127\n",
            " 18.20422535 18.24823944 18.29225352 18.33626761 18.38028169 18.42429577\n",
            " 18.46830986 18.51232394 18.55633803 18.60035211 18.6443662  18.68838028\n",
            " 18.73239437 18.77640845 18.82042254 18.86443662 18.9084507  18.95246479\n",
            " 18.99647887 19.04049296 19.08450704 19.12852113 19.17253521 19.2165493\n",
            " 19.26056338 19.30457746 19.34859155 19.39260563 19.43661972 19.4806338\n",
            " 19.52464789 19.56866197 19.61267606 19.65669014 19.70070423 19.74471831\n",
            " 19.78873239 19.83274648 19.87676056 19.92077465 19.96478873 20.00880282\n",
            " 20.0528169  20.09683099 20.14084507 20.18485915 20.22887324 20.27288732\n",
            " 20.31690141 20.36091549 20.40492958 20.44894366 20.49295775 20.53697183\n",
            " 20.58098592 20.625      20.66901408 20.71302817 20.75704225 20.80105634\n",
            " 20.84507042 20.88908451 20.93309859 20.97711268 21.02112676 21.06514085\n",
            " 21.10915493 21.15316901 21.1971831  21.24119718 21.28521127 21.32922535\n",
            " 21.37323944 21.41725352 21.46126761 21.50528169 21.54929577 21.59330986\n",
            " 21.63732394 21.68133803 21.72535211 21.7693662  21.81338028 21.85739437\n",
            " 21.90140845 21.94542254 21.98943662 22.0334507  22.07746479 22.12147887\n",
            " 22.16549296 22.20950704 22.25352113 22.29753521 22.3415493  22.38556338\n",
            " 22.42957746 22.47359155 22.51760563 22.56161972 22.6056338  22.64964789\n",
            " 22.69366197 22.73767606 22.78169014 22.82570423 22.86971831 22.91373239\n",
            " 22.95774648 23.00176056 23.04577465 23.08978873 23.13380282 23.1778169\n",
            " 23.22183099 23.26584507 23.30985915 23.35387324 23.39788732 23.44190141\n",
            " 23.48591549 23.52992958 23.57394366 23.61795775 23.66197183 23.70598592\n",
            " 23.75       23.79401408 23.83802817 23.88204225 23.92605634 23.97007042\n",
            " 24.01408451 24.05809859 24.10211268 24.14612676 24.19014085 24.23415493\n",
            " 24.27816901 24.3221831  24.36619718 24.41021127 24.45422535 24.49823944\n",
            " 24.54225352 24.58626761 24.63028169 24.67429577 24.71830986 24.76232394\n",
            " 24.80633803 24.85035211 24.8943662  24.93838028 24.98239437 25.02640845\n",
            " 25.07042254 25.11443662 25.1584507  25.20246479 25.24647887 25.29049296\n",
            " 25.33450704 25.37852113 25.42253521 25.4665493  25.51056338 25.55457746\n",
            " 25.59859155 25.64260563 25.68661972 25.7306338  25.77464789 25.81866197\n",
            " 25.86267606 25.90669014 25.95070423 25.99471831 26.03873239 26.08274648\n",
            " 26.12676056 26.17077465 26.21478873 26.25880282 26.3028169  26.34683099\n",
            " 26.39084507 26.43485915 26.47887324 26.52288732 26.56690141 26.61091549\n",
            " 26.65492958 26.69894366 26.74295775 26.78697183 26.83098592 26.875\n",
            " 26.91901408 26.96302817 27.00704225 27.05105634 27.09507042 27.13908451\n",
            " 27.18309859 27.22711268 27.27112676 27.31514085 27.35915493 27.40316901\n",
            " 27.4471831  27.49119718 27.53521127 27.57922535 27.62323944 27.66725352\n",
            " 27.71126761 27.75528169 27.79929577 27.84330986 27.88732394 27.93133803\n",
            " 27.97535211 28.0193662  28.06338028 28.10739437 28.15140845 28.19542254\n",
            " 28.23943662 28.2834507  28.32746479 28.37147887 28.41549296 28.45950704\n",
            " 28.50352113 28.54753521 28.5915493  28.63556338 28.67957746 28.72359155\n",
            " 28.76760563 28.81161972 28.8556338  28.89964789 28.94366197 28.98767606\n",
            " 29.03169014 29.07570423 29.11971831 29.16373239 29.20774648 29.25176056\n",
            " 29.29577465 29.33978873 29.38380282 29.4278169  29.47183099 29.51584507\n",
            " 29.55985915 29.60387324 29.64788732 29.69190141 29.73591549 29.77992958\n",
            " 29.82394366 29.86795775 29.91197183 29.95598592 30.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1_EbFlqzjAT",
        "colab_type": "text"
      },
      "source": [
        "Write a single mathematical expression describing the relationship you found between $n$ (the size of the dataset) and $k$ (the number of datapoints used to classify each validation datum).\n",
        "\n",
        "(YOUR ANSWER HERE) 0.04n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-vDZy0F3eyeH"
      },
      "source": [
        "## Problem 3\n",
        "Now, we will **be writing our k-NNA**. Recall that we said a kNN is comprised of a predictions and using those predictions to classify the data. Here we will try to mimic sklearn's kNN methods. We will be using the Pima diabetes dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YFfjG9G3lVO",
        "colab_type": "text"
      },
      "source": [
        "### Loading and splitting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zbs8WICFgITd",
        "colab": {}
      },
      "source": [
        "# -- loading dataset -- #\n",
        "url = \"https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week1/diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "data = po.read_csv(url, names=names)\n",
        "\n",
        "# -- dropping NaN rows -- #\n",
        "invalid = ['plas', 'pres', 'skin', 'test', 'mass']\n",
        "\n",
        "for i in invalid:\n",
        "    data[i].replace(to_replace=0, value=np.nan, inplace=True)\n",
        "    \n",
        "data = data.dropna(axis=0).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSUwHL6-4P2F",
        "colab_type": "text"
      },
      "source": [
        "Now, let's clearly define which columns will act as explanatory variables, and which column will be the target value, and split the dataset between your training data and testing data. Let's try an 80-20 split and use sklearn's [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method (set random_state = 0 so we get the same output each time)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9MXZjxRcgy78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "56f550fb-a51d-4a2a-df7d-da6537907cc7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# columns we will use to make predictions with (features!) feel free to play around with these\n",
        "X_cols = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
        "# column that we want to predict\n",
        "y_col = 'class'\n",
        "\n",
        "\n",
        "# split X and y into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[X_cols], data[y_col], test_size=0.2, random_state=0)\n",
        "\n",
        "# further split X and y of training into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "print('There are {} training samples with {} features and {} associated classification labels'.format(*X_train.shape, *y_train.shape))\n",
        "print('There are {} validation samples with {} features and {} associated classification labels'.format(*X_val.shape, *y_val.shape))\n",
        "print('There are {} test samples with {} features and {} associated classification labels'.format(*X_test.shape, *y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 250 training samples with 8 features and 250 associated classification labels\n",
            "There are 63 validation samples with 8 features and 63 associated classification labels\n",
            "There are 79 test samples with 8 features and 79 associated classification labels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "De_EJnYKgz_6"
      },
      "source": [
        "### Normalizing Data\n",
        "\n",
        "Let's not forget to normalize the data! We'll use sklearn's StandardScaler normalization like we did before to normalize the training **and** validation/data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6PD6-ibriBJO",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for i in list(X_train):\n",
        "    feature_data_train = X_train[i].values.reshape(-1, 1)\n",
        "    scaler.fit(feature_data_train)\n",
        "    X_train[i] = scaler.transform(feature_data_train)\n",
        "\n",
        "for j in list(X_test):\n",
        "    feature_data_test = X_test[j].values.reshape(-1, 1)\n",
        "    scaler.fit(feature_data_test)\n",
        "    X_test[j] = scaler.transform(feature_data_test)\n",
        "    \n",
        "for k in list(X_val):\n",
        "    feature_data_val = X_val[k].values.reshape(-1, 1)\n",
        "    scaler.fit(feature_data_val)\n",
        "    X_val[k] = scaler.transform(feature_data_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hnv61aiiitxU"
      },
      "source": [
        "### Writing our kNN\n",
        "\n",
        "Now for the fun part! Fill in the 3 following methods, euclidean_dist(), predict(), and knn().\n",
        "\n",
        "The predict method that we'll make below needs to: \n",
        "1. Compute the euclidean distance between the “new” observation and all the data points in the training set. \n",
        "2. Assign the corresponding label to the observation\n",
        "3. Select the k nearest ones and perform a \"majority vote\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXkIw6zN3lVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Euclidean distance function from tutorial\n",
        "def euclidean_dist(datum1, datum2):\n",
        "    inner_val = 0.0\n",
        "    \n",
        "    for g in range(datum1.shape[0]):\n",
        "        inner_val += (datum1[g]- datum2[g]) ** 2\n",
        "    \n",
        "    distance = np.sqrt(inner_val)\n",
        "    return(distance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FqJkm_ytjFgM",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def predict(x_training, y_training, x_test_sample, k):\n",
        "    # create list for distances and targets\n",
        "    distances = []\n",
        "    targets = []\n",
        "\n",
        "    ## YOUR CODE HERE\n",
        "    i = 0\n",
        "    while i < len(y_training):\n",
        "      targets.append(y_training[i])\n",
        "      i += 1\n",
        "    i = 0\n",
        "    while i < len(x_training):\n",
        "      j = 0\n",
        "      while j < len(x_test_sample):\n",
        "        distances.append(euclidean_dist(x_training[i], x_test_sample[j]))\n",
        "        targets.append(y_training[i])\n",
        "        j += 1\n",
        "      i += 1\n",
        "    return(distances, targets)\n",
        "    \n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BguZLcRa3lVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def knn(x_training, y_training, x_testing, k):\n",
        "    \n",
        "    ## YOUR CODE HERE\n",
        "    predict_result = predict(x_training, y_training, x_testing, k)\n",
        "    distances =  predict_result[0]\n",
        "    targets = predict_result[1]\n",
        "    indexes = sorted(range(len(distances)), key = lambda sub: test_list[sub])[:k]\n",
        "    y_training_neighbors = []\n",
        "    for value in indexes:\n",
        "      y_training_neighbors.append(targets[value])\n",
        "    prediction = max(set(y_training_neighbors), key=output_values.count)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4uhQZPIpjdo9"
      },
      "source": [
        "When done, test your code by running the methods here!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AHRJXVr7jcao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "7f1fedb3-3f38-4873-f68a-fc570633ee74"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "predictions_slow = knn(X_train, y_train, X_val, k=5)\n",
        "\n",
        "print('Took {} seconds'.format(time.time() - start))\n",
        "print(\"Validation Accuracy is \", accuracy_score(y_val,predictions_slow)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-c8ce05bea58f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpredictions_slow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Took {} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-e638a95a302d>\u001b[0m in \u001b[0;36mknn\u001b[0;34m(x_training, y_training, x_testing, k)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m## YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredict_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_testing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpredict_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-44b6013b2008>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(x_training, y_training, x_test_sample, k)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4403\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4404\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4405\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4406\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a51RcbJ3lVq",
        "colab_type": "text"
      },
      "source": [
        "Check sklearn's predictions on validation data from the tutorial notebook and make sure they match yours. Sklearn is faster, but you should get the same answers."
      ]
    }
  ]
}