{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JP Transfer Learning Draft 2_NASA Jr. Week 3_pickling_version.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anaaparamesh/Week1_Public/blob/master/JP_Transfer_Learning_Draft_2_NASA_Jr_Week_3_pickling_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ukBWGU0O9l",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJRhXLZlHgAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.is_gpu_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNtIxtyy5qS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run this cell to import the packages you will need to unpack the dataset\n",
        "# File manipulation and IO (input/output)\n",
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# Import numerical and dataframe handling\n",
        "import numpy as np\n",
        "import scipy\n",
        "import pandas as pd\n",
        "\n",
        "# Data preprocessing\n",
        "from PIL import Image\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Model scoring\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "# Import standard machine learning machinery\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Garbage collection (for saving RAM during training)\n",
        "import gc\n",
        "\n",
        "# Import plotting functionality\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "import matplotlib\n",
        "\n",
        "from skimage.color import gray2rgb\n",
        "from skimage import img_as_ubyte\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbWAIUdOrF01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set plotting preferences\n",
        "%matplotlib inline\n",
        "font = {'family' : 'sans-serif',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 16}\n",
        "matplotlib.rc('font', **font)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkpOzgA9w7jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the path to the Week 3 challenge data on GitHub\n",
        "github_data_path = 'https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/ChallengeProjects/Week3/'\n",
        "\n",
        "# Download 3 zip files from github containing training, validation, and test images\n",
        "os.system('wget '+ os.path.join(github_data_path, 'Mamm_Images_Test.zip'))\n",
        "os.system('wget '+ os.path.join(github_data_path, 'Mamm_Images_Train.zip'))\n",
        "os.system('wget '+ os.path.join(github_data_path, 'Mamm_Images_Val.zip'))\n",
        "\n",
        "# Download the binary and multiclass labels for the \n",
        "os.system('wget ' + os.path.join(github_data_path,'train_binary_labels.csv'))\n",
        "os.system('wget ' + os.path.join(github_data_path,'train_multiclass_labels.csv'))\n",
        "os.system('wget ' + os.path.join(github_data_path,'val_binary_labels.csv'))\n",
        "os.system('wget ' + os.path.join(github_data_path,'val_multiclass_labels.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZH9ZTL6y6NY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Unzip the image data and populate dataframe objects ###\n",
        "# Unzip all files to respective folders in current directory\n",
        "zip_ref = zipfile.ZipFile('Mamm_Images_Train.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile('Mamm_Images_Val.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile('Mamm_Images_Test.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRXbNS6a1ncH",
        "colab_type": "text"
      },
      "source": [
        "### Store Images and Labels in Numpy Arrays and Dataframes\n",
        "\n",
        "The following cell will load the images and labels into a pair of numpy ndarray and pandas dataframe objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvoo9zFr5rTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use pd.read_csv to open csv file contents to pandas dataframes\n",
        "train_binary_labels_df = pd.read_csv('train_binary_labels.csv', header=None)\n",
        "train_multiclass_labels_df = pd.read_csv('train_multiclass_labels.csv', header=None)\n",
        "val_binary_labels_df = pd.read_csv('val_binary_labels.csv', header=None)\n",
        "val_multiclass_labels_df = pd.read_csv('val_multiclass_labels.csv', header=None)\n",
        "\n",
        "# Concatenate train/validation labels into one set\n",
        "#(you may decide to separate later for your own train/val/mocktest split)\n",
        "train_binary_labels_df = pd.concat([train_binary_labels_df, val_binary_labels_df], axis=0, ignore_index = True)\n",
        "train_multiclass_labels_df = pd.concat([train_multiclass_labels_df, val_multiclass_labels_df], axis=0, ignore_index = True)\n",
        "\n",
        "# Add columns to train_binary_labels_df dataframe that contains unique\n",
        "# (original) indices of train data\n",
        "unique_indices_df = pd.DataFrame(list(range(5500)))\n",
        "train_binary_labels_df = pd.concat([train_binary_labels_df, unique_indices_df], axis=1)\n",
        "train_binary_labels_df.columns = ['Label', 'Unique_Index']\n",
        "train_multiclass_labels_df = pd.concat([train_multiclass_labels_df, unique_indices_df], axis=1)\n",
        "train_multiclass_labels_df.columns = ['Label', 'Unique_Index']\n",
        "\n",
        "# Load images from file and save to both\n",
        "# 1) dataframe objects\n",
        "# 2) numpy arrays of shape (num_examples, num_pixels_wide, num_pixels_high)\n",
        "\n",
        "# Read in train images (from both train and val directories) to ndarray\n",
        "train_images = np.zeros((5500,299,299), dtype=np.uint8)\n",
        "for ind in range(5000):\n",
        "  im = plt.imread('Mamm_Images_Train/image' + str(ind) + '.jpg')\n",
        "  train_images[ind, :, :] = im\n",
        "  \n",
        "for ind in range(500):\n",
        "  im = plt.imread('Mamm_Images_Val/image' + str(ind) + '.jpg')\n",
        "  train_images[ind + 5000, :, :] = im\n",
        "\n",
        "# Read in test images to ndarray\n",
        "test_images = np.zeros((1500,299,299), dtype=np.uint8)\n",
        "for ind in range(1500):\n",
        "  im = plt.imread('Mamm_Images_Test/image' + str(ind) + '.jpg')\n",
        "  test_images[ind, :, :] = im\n",
        "\n",
        "# Make dataframes that contain the same information as the ndarrays\n",
        "# The N-th row contains a 299x299 ndarray, and a unique index for that image\n",
        "train_images_df = pd.DataFrame([[train_images[i,:,:] for i in range(train_images.shape[0])]])\n",
        "train_images_df = train_images_df.transpose()\n",
        "unique_indices_df = pd.DataFrame(list(range(5500)), dtype = np.int)\n",
        "train_images_df = pd.concat([train_images_df, unique_indices_df], axis = 1)\n",
        "train_images_df.columns = ['Images', 'Unique_Index']\n",
        "\n",
        "test_images_df = pd.DataFrame([[test_images[i,:,:] for i in range(test_images.shape[0])]])\n",
        "test_images_df = test_images_df.transpose()\n",
        "unique_indices_df = pd.DataFrame(list(range(5500, 7000)), dtype = np.int)\n",
        "test_images_df = pd.concat([test_images_df, unique_indices_df], axis = 1)\n",
        "test_images_df.columns = ['Images', 'Unique_Index']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agnm12vvmXmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import skimage.transform as image_transform\n",
        "\n",
        "# Grab the original shape of the images\n",
        "shape_train_images = train_images.shape\n",
        "num_images = shape_train_images[0]\n",
        "\n",
        "# Specify a desired resizing shape.\n",
        "# NOTE: This can be modified to trade-off between training-speed and performance\n",
        "resize_shape = (150, 150)\n",
        "\n",
        "# Initialize an array for storing the resized images\n",
        "training_images = np.zeros((num_images,) + resize_shape)\n",
        "\n",
        "# Loop over each image in the data and perform a resizing operation\n",
        "for img_num in range(num_images):\n",
        "    training_images[img_num] = image_transform.resize(train_images[img_num, :, :], resize_shape)\n",
        "\n",
        "train_images = training_images\n",
        "\n",
        "\n",
        "shape_test_images = test_images.shape\n",
        "num_images_test = shape_test_images[0]\n",
        "\n",
        "# Specify a desired resizing shape.\n",
        "# NOTE: This can be modified to trade-off between training-speed and performance\n",
        "resize_shape = (150, 150)\n",
        "\n",
        "# Initialize an array for storing the resized images\n",
        "testing_images = np.zeros((num_images_test,) + resize_shape)\n",
        "\n",
        "# Loop over each image in the data and perform a resizing operation\n",
        "for img_num in range(num_images_test):\n",
        "    testing_images[img_num] = image_transform.resize(train_images[img_num, :, :], resize_shape)\n",
        "\n",
        "test_images = testing_images\n",
        "# Remove the full-resolution versions from memory (just glogging things up)\n",
        "del shape_train_images\n",
        "del num_images \n",
        "del training_images\n",
        "del shape_test_images\n",
        "del num_images_test \n",
        "del testing_images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8MjAxxREoFb",
        "colab_type": "text"
      },
      "source": [
        "# Example binary CNN classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHBjD8AxquIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data normalization\n",
        "if train_images[0,::].max() > 1:\n",
        "    train_images = train_images.astype(np.float32)/255.0 # <-- Apply normalization on this line\n",
        "if test_images[0,::].max() > 1:\n",
        "    test_images = test_images.astype(np.float32)/255.0 # <-- Apply normalization on this line\n",
        "\n",
        "\n",
        "#add an axis, then convert from grayscale to rgb\n",
        "train_images = train_images[:,:,:,np.newaxis]\n",
        "test_images = test_images[:,:,:, np.newaxis]\n",
        "original_test = tf.constant(test_images)\n",
        "original = tf.constant(train_images)\n",
        "converted = tf.image.grayscale_to_rgb(original)\n",
        "converted_test = tf.image.grayscale_to_rgb(original_test)\n",
        "train_images = converted.numpy()\n",
        "test_images = converted_test.numpy()\n",
        "train_images.shape\n",
        "del original\n",
        "del converted\n",
        "del original_test\n",
        "del converted_test\n",
        "print(test_images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LeuKhijbBVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Create one-hot labels\n",
        "train_binary_labels = train_binary_labels_df[\"Label\"]\n",
        "\n",
        "#convert labels to onehot, ensure type is float32\n",
        "train_binary_labels = tf.keras.utils.to_categorical(train_binary_labels, 2).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu2FiweZGHL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Shuffle and partition labeled data\n",
        "train_images_shuffled, train_binary_labels_shuffled = shuffle(train_images, train_binary_labels, random_state = 25, stratify = train_multiclass_labels_df[\"Label\"])\n",
        "\n",
        "val_size = 1000\n",
        "mocktest_size = 500\n",
        "\n",
        "mocktest_data = train_images_shuffled[0:mocktest_size, :, :]\n",
        "mocktest_binary_labels = train_binary_labels_shuffled[0:mocktest_size, :]\n",
        "\n",
        "val_data = train_images_shuffled[mocktest_size:mocktest_size+val_size, :, :]\n",
        "val_binary_labels = train_binary_labels_shuffled[mocktest_size:mocktest_size+val_size, :]\n",
        "\n",
        "\n",
        "partial_train_data = train_images_shuffled[mocktest_size+val_size:,:,:]\n",
        "tr_binary_labels = train_binary_labels_shuffled[mocktest_size+val_size:,:]\n",
        "\n",
        "\n",
        "#creating tf objects and deleting np arrays so less memory no 2 copies in aray.\n",
        "mocktest_tf = tf.constant(mocktest_data, dtype=tf.float16)\n",
        "val_data_tf = tf.constant(val_data, dtype=tf.float16)\n",
        "partial_train_data_tf = tf.constant(partial_train_data, dtype=tf.float16)\n",
        "\n",
        "del mocktest_data, val_data, partial_train_data, train_images\n",
        "\n",
        "mocktest_binary_labels_tf = tf.constant(mocktest_binary_labels, dtype=tf.float16)\n",
        "val_binary_labels_tf = tf.constant(val_binary_labels, dtype=tf.float16)\n",
        "tr_binary_labels_tf = tf.constant(tr_binary_labels, dtype=tf.float16)\n",
        "\n",
        "del mocktest_binary_labels, val_binary_labels, tr_binary_labels\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYPIMNK2xt6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data augmentation layers\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(0.3),\n",
        "        layers.experimental.preprocessing.RandomContrast(1.0),\n",
        "        layers.experimental.preprocessing.PreprocessingLayer()\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iN6nwkyeW_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#inceptionv3\n",
        "base_model = tf.keras.applications.InceptionV3(input_shape=(partial_train_data_tf.shape[1:]),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = False\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "\n",
        "# Define some functions for experimenting with unfreezing layers\n",
        "def unfreeze_layers(model, top_n_layers):\n",
        "    \"\"\"Makes the top layers of the `base_model` trainable (except batch norm)\"\"\"\n",
        "    # We unfreeze the top layers while leaving BatchNorm layers frozen\n",
        "    for layer in model.layers[-top_n_layers:]:\n",
        "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "            layer.trainable = True\n",
        "\n",
        "def freeze_layers(model, top_n_layers):\n",
        "    \"\"\"Makes the top layers of the `base_model` untrainable\"\"\"\n",
        "    # We unfreeze the top layers while leaving BatchNorm layers frozen\n",
        "    for layer in model.layers[-top_n_layers:]:\n",
        "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "            layer.trainable = False\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kik_Lf1IfMWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# layers defined\n",
        "prediction_layer1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "#d_r = .5\n",
        "#drop_out_layer = tf.keras.layers.Dropout(rate=d_r)\n",
        "prediction_layer2 = tf.keras.layers.Dense(2, activation='softmax')\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "# Build the final model as a sequence:\n",
        "# input --> feature extractor --> global pooling --> classifier --> predictions\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    global_average_layer,\n",
        "    prediction_layer1,\n",
        "    #drop_out_layer,\n",
        "    prediction_layer2\n",
        "])\n",
        "\n",
        "#playing with unfreezing layers\n",
        "n_layers = 15\n",
        "unfreeze_layers(model, n_layers)\n",
        "\n",
        "# Specify the loss function to use\n",
        "loss_func = tf.keras.losses.binary_crossentropy\n",
        "\n",
        "# Use the RMSprop learning algorithm to optimize the network weights\n",
        "base_learning_rate = 0.0001\n",
        "opt = tf.keras.optimizers.RMSprop(lr=base_learning_rate)\n",
        "\n",
        "# Compile the model using the specified loss function and potimizer\n",
        "model.compile(loss=loss_func, optimizer=opt, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syf9KdnlfSyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is called after each epoch\n",
        "# (It will ensure that your training process does not consume all available RAM)\n",
        "\n",
        "#keep getting errors here:     ValueError: Input 0 of layer conv2d_188 is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape [None, 75, 75, 1]\n",
        "\n",
        "class garbage_collect_callback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    gc.collect()\n",
        "\n",
        "# Time how long it takes the model to train for these epochs\n",
        "start_time = time.time()\n",
        "\n",
        "# Perform the training method\n",
        "history = model.fit(partial_train_data_tf,\n",
        "                    tr_binary_labels_tf,\n",
        "                    batch_size=32,\n",
        "                    epochs= 12,\n",
        "                    verbose=True,\n",
        "                    validation_data=(val_data_tf, val_binary_labels_tf),\n",
        "                    callbacks = [garbage_collect_callback(), tf.keras.callbacks.EarlyStopping(patience = 5, restore_best_weights=True)])\n",
        "\n",
        "\n",
        "stop_time = time.time()\n",
        "print(\"--- %s seconds ---\" % (stop_time - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMitjKbGfbcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot model train/validation accuracy and model train/validation loss\n",
        "# Summarize history for accuracy\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Summarize history for loss\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b7MS4h1DNiT",
        "colab_type": "text"
      },
      "source": [
        "## Example code showing calculation of evaluation metrics (binary classifier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQq_lY-ccStc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ground_truth_onehot = mocktest_binary_labels_tf\n",
        "ground_truth_labels = np.array([np.argmax(ground_truth_onehot[i, :])\n",
        "                               for i in range(mocktest_binary_labels_tf.shape[0])])\n",
        "\n",
        "predicted_onehot = model.predict(mocktest_tf)\n",
        "predicted_labels = np.array([np.argmax(predicted_onehot[i, :]) \n",
        "                            for i in range(predicted_onehot.shape[0])], dtype = np.int8) \n",
        "\n",
        "##--Confusion Matrix Score for Binary Classifier--\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "binary_confmat_weights = np.array([[2, -3],\n",
        "                                  [-6, 2]])\n",
        "confmat = confusion_matrix(ground_truth_labels, predicted_labels)\n",
        "print(confmat)\n",
        "\n",
        "# Simply multiply two matrices elementwise, and then sum all the results\n",
        "confmat_score = np.sum(confmat * binary_confmat_weights)\n",
        "\n",
        "#--ROCAUC for Binary Classifier--\n",
        "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(ground_truth_labels, predicted_labels, pos_label=1)\n",
        "roc_auc = metrics.auc(false_positive_rate, true_positive_rate)    \n",
        "print(f'Confusion Matrix Score: {confmat_score}, ROCAUC: {roc_auc:.3f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WmcuzMdgVSN",
        "colab_type": "text"
      },
      "source": [
        "## Example code showing calculation of evaluation metrics (multiclass classifier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWzmgwePgW0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use ground_truth_labels and predicted_labels to calculate confusion matrix\n",
        "#\n",
        "# Note: When your model is tested, the ground_truth_labels will be the test_labels\n",
        "# and the predicted_labels will be your models predicted labels on the test set.\n",
        "# To demonstrate the calculation however, we will use other data to populate \n",
        "# ground_truth_labels and predicted_labels.\n",
        "#\n",
        "# Variable definitions\n",
        "#-------------------------------------\n",
        "# ground_truth_labels - 1D array where ith element is an integer representing \n",
        "# the true label of example i\n",
        "#\n",
        "# predicted_labels - 1D array where ith element is an integer representing the\n",
        "# predicted class of example i\n",
        "\n",
        "# Because a classifier wasn't trained for the multiclass problem (in the starter\n",
        "# code anyway) we will quickly define ground_truth_labels and predicted_labels\n",
        "# with dummy values\n",
        "\n",
        "ground_truth_labels = np.array([i for i in range(1500)])//300 #Labels range from 0 to 4\n",
        "ground_truth_labels_onehot = np.eye(5)[ground_truth_labels.astype(np.uint8)]\n",
        "\n",
        "predicted_labels = np.zeros((1500,), dtype = np.uint8) #prediction is always class 0 (this is a very naiive predictor)\n",
        "prediction_scores = np.eye(5)[predicted_labels] #Row i of this matrix is the likelyhood 'score' for example i in each of the classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3HyjrjKoYBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--Confusion Matrix Score for Multiclass Classifier--\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "multiclass_confmat_weights = np.array([[2,-1,-1,-3,-3],\n",
        "                                    [-2,2,-1,-3,-3],\n",
        "                                    [-2,-1,2,-3,-3],\n",
        "                                    [-6,-4,-4,2,-2],\n",
        "                                    [-6,-4,-4,-2,2]])\n",
        "\n",
        "confmat = confusion_matrix(ground_truth_labels, predicted_labels)\n",
        "  \n",
        "confmat_score = np.sum(confmat * multiclass_confmat_weights)\n",
        "\n",
        "\n",
        "      \n",
        "#--ROCAUC for Multiclass Classifier--\n",
        "  # ROCAUC for multiclass classifiers are calculated by treating each class as a binary classification (this class or all other classes) and micro-averaging \n",
        "  # each class's ROCAUC to get a total ROCAUC\n",
        "false_positive_rate = dict()\n",
        "true_positive_rate = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(5):  # looping through each of 5 classes and getting false and true positive rates and calculating ROCAUC for each class\n",
        "  false_positive_rate[i], true_positive_rate[i], _ = metrics.roc_curve(ground_truth_labels_onehot[:,i], prediction_scores[:,i]) # Because labels are not binary, \n",
        "  roc_auc[i] = metrics.auc(false_positive_rate[i], true_positive_rate[i])                                                    # they need to be One Hot Vectors (ex. 2 = [0,0,1,0,0])\n",
        "                                                                                                                     # prediction_scores are the predicted 'probability'\n",
        "                                                                                                                     # that the example belongs to each of the 5 classes\n",
        "false_positive_rate[\"micro\"], true_positive_rate[\"micro\"], _ = metrics.roc_curve(ground_truth_labels_onehot.ravel(), prediction_scores.ravel())\n",
        "roc_auc = metrics.auc(false_positive_rate[\"micro\"], true_positive_rate[\"micro\"]) # micro-averaging averages the ROCAUCs from each class weighted \n",
        "                                                                         # according to number of example in that class\n",
        "   \n",
        "print(f'Multiclass Confusion Matrix Score: {confmat_score}, c: {roc_auc:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ultum1UWTdG7",
        "colab_type": "text"
      },
      "source": [
        "# Submitting your model\n",
        "\n",
        "Once you have finished creating your classifier, you need to use it to classify the test images and make a pickle file containing your models predictions stored as a pandas dataframe. Instructions for how to build the pandas dataframe containing your model's predictions are included below. (Note that the process is demonstrated for the example classifier in the next code cell to help clarify these instructions.)\n",
        "\n",
        "* Create pandas dataframe with 4 columns (or 7 columns for complex lables). One column should contain your model's prediction for each image (represented as an integer) and one column should contain the unique index for that image in the test data (recall that test data indices range from 5500 to 5999). The other columns in the dataframe should contain the probability that the image is classified in each class. **Make sure the unique indices on your dataframe match the unique indices of the images being predicted. The order of rows and columns do not matter, as long as there are 1500 rows and either 4 or 7 columns (depending on which classifier you have made)**. (The below cell demonstrates how to build this dataframe created using the example classifier. You can refer to this code for more clarification of these instructions.)\n",
        " * If your classifier does not compute the probabilities for each class and simply outputs an integer label for the prediction (e.g. 0, 1, 2, 3, or 4 for multiclass classification), the probability for the predicted class should be 1.00 and the probability for all other class(es) should be 0.00 in the dataframe.\n",
        "* Download the pandas dataframe as a pickle file using pickle.dump() and files.download(). (See below cell for clarification on this procedure.)\n",
        "* Submit your pickle file to the submissions channel on the Medlytics 2020 discord channel. You can submit up to 3 times before your final evaluation.\n",
        "\n",
        "Your model will be evaluated on confusion matrix score (explained below), area under the ROC curve (ROCAUC), and creativity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8phZzYxs5rm",
        "colab_type": "text"
      },
      "source": [
        "# Example code for saving model predictions on the test set (for submission)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntbJLmh-CrBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Use the created neural network model to create a set of predictions for the test set\n",
        "\n",
        "#Refer to the dataframe produced by this cell to clarify correct formatting when\n",
        "#submitting pickle files for week 3 challenge (This cell produces dataframe\n",
        "#using the example CNN classifier, but the same dataframe should be produced for\n",
        "#any type of classifier.)\n",
        "\n",
        "#use model.predict to create a set of predictions (onehot encoded)\n",
        "test_predictions_onehot = model.predict(test_images[:,:,:,:])\n",
        "\n",
        "def create_test_predictions_df_for_submission(test_predictions_onehot):\n",
        "    test_predictions_df = pd.DataFrame(test_predictions_onehot)\n",
        "\n",
        "    #add a column to dataframe containing the unique index for each sample\n",
        "    test_predictions_df = pd.concat((test_predictions_df, test_images_df[\"Unique_Index\"]), axis = 1)\n",
        "\n",
        "    #add another column to dataframe with an integer representing the guessed class for that example\n",
        "    #(Note this is the number corresponding to the class with highest probability score)\n",
        "    guesses = [np.argmax(test_predictions_onehot[i,:]) for i in range(test_predictions_onehot.shape[0])]\n",
        "    test_predictions_df = pd.concat((test_predictions_df, pd.DataFrame(guesses, columns = ['predicted_class'])), axis = 1)\n",
        "\n",
        "    return test_predictions_df\n",
        "\n",
        "test_predictions_df = create_test_predictions_df_for_submission(test_predictions_onehot)\n",
        "print('This is what the appropriately formatted dataframe should look like:')\n",
        "print('(4 columns total for binary classification, and 7 for multiclass classification)\\n')\n",
        "print(test_predictions_df.head())\n",
        "print('\\n\\n')\n",
        "\n",
        "fname = 'what_have_the_romans_ever_done_for_us.p'\n",
        "pickle.dump(test_predictions_df, open(fname, 'wb')) # Creates pickle file in Google Colab but doesn't save to your machine\n",
        "    \n",
        "# Saving to pickle file to local drive (should be in Downloads). \n",
        "# Submit via the Medlytics 2020 \"submissions\" channel on Discord\n",
        "files.download(fname) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkJcTaUvc2uG",
        "colab_type": "text"
      },
      "source": [
        "# Troubleshooting tips\n",
        "\n",
        "- If you try to plot the images using matplotlib  you may notice that it will invert the grayscale values when it plots. You can fix this by plotting 255 - pixel_values or by adding the argument `cmap='gray'` to your imshow function (as I did when visualizing the images).\n",
        "\n",
        "- If you get an \"ResourcesExhausted\" error, this means you have used all the GPU resources dedicated to your session and you will need to restart your runtime (Runtime > Restart Runtime). After restarting your runtime your environment will be wiped so you will need to reload the data.\n",
        "\n",
        "- If you get a warning window that says you are close to reaching the session's memory limit, this means you are nearing a ResourcesExhausted error. You can still run code until the error occurs, but note that you will likely need to restart your runtime soon.\n",
        "\n",
        "- ResourcesExhausted errors will only occur if you're using GPU support.\n",
        "\n",
        "- Google Colab will end your runtime automatically after 24 hours\n",
        "\n",
        "- If things seem really slow, double check that you're using the GPU\n",
        "\n",
        "- If you are close to using all available ram, try to double check the memory being consumed by different variables using the provided cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyavuR7Gz8nP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you find yourself running out of ram below is a useful method to print out \n",
        "# the sizes of the variables in your workspace\n",
        "\n",
        "import sys\n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
        "                         key= lambda x: -x[1])[:10]:\n",
        "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
        "\n",
        "type(train_images[0,:,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihlKbbOCqyQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}